# Local vLLM API Configuration Example
# Start vLLM service first: python -m vllm.entrypoints.openai.api_server --model /path/to/model --port 8000

# ==================== Model Configuration ====================
model_to_evaluate: "openai/Qwen2.5-Coder-32B-Instruct"  # openai/ prefix indicates using vLLM
use_api: true

# ==================== API Configuration ====================
openai:
  api_key: "dummy-key"  # vLLM doesn't need real API key
  base_url: "http://localhost:8000/v1"  # vLLM service address
  model: "Qwen2.5-Coder-32B-Instruct"
  max_tokens: 8192
  temperature: 0.7
  
# Static aesthetics judge model settings
static_judge:
  model: "gpt-4o"
  api_key: ${OPENAI_API_KEY}
  base_url: null
  temperature: 0.0
  max_tokens: 4096

# Interactive score judge model settings
interactive_judge:
  model: "gpt-4o"
  api_key: ${OPENAI_API_KEY}
  base_url: null
  temperature: 0.0
  max_tokens: 4096

# ==================== Generation Configuration ====================
generation:
  temperature: 0.7
  max_tokens: 8192
  num_threads: 16  # vLLM concurrency can be reduced appropriately

# ==================== Local Inference Configuration ====================
local_inference:
  model_path: ""
  tensor_parallel_size: 1
  max_model_len: 10240
  cuda_devices: "0"

# ==================== Benchmark Configuration ====================
benchmark:
  bench_name: "opendesign"
  data_file: "benchmark_data/all_prompt.jsonl"
  output_dir: "arena-bench-result"

# ==================== Screenshot Configuration ====================
screenshot:
  max_workers: 16
  timeout: 120
  batch_size: 100

# ==================== Agent Score Configuration ====================
agent_score:
  max_iter: 7
  window_width: 1920
  window_height: 1200
  timeout_seconds: 600
  batch_size: 3
  max_workers: 16

