# Local Inference Configuration Example
# Use vLLM to directly load model for local inference (no need to start API service)

# ==================== Model Configuration ====================
model_to_evaluate: "Qwen/Qwen2.5-Coder-32B-Instruct"
use_api: false  # Use local inference mode

# ==================== API Configuration ====================
openai:
  api_key: ${OPENAI_API_KEY}
  base_url: null
  model: "gpt-4o"
  max_tokens: 8192
  temperature: 0.7

# Static aesthetics judge model settings
static_judge:
  model: "gpt-4o"
  api_key: ${OPENAI_API_KEY}
  base_url: null
  temperature: 0.0
  max_tokens: 4096

# Interactive score judge model settings
interactive_judge:
  model: "gpt-4o"
  api_key: ${OPENAI_API_KEY}
  base_url: null
  temperature: 0.0
  max_tokens: 4096

# ==================== Generation Configuration ====================
generation:
  temperature: 0.7
  max_tokens: 8192
  num_threads: 1  

# ==================== Local Inference Configuration ====================
local_inference:
  model_path: "Qwen/Qwen2.5-Coder-32B-Instruct"  # HuggingFace model name or local path
  tensor_parallel_size: 4  # Use 4 GPUs
  max_model_len: 10240
  cuda_devices: "0,1,2,3"  # Use GPU 0-3

# ==================== Benchmark Configuration ====================
benchmark:
  bench_name: "opendesign"
  data_file: "benchmark_data/all_prompt.jsonl"
  output_dir: "arena-bench-result"

# ==================== Screenshot Configuration ====================
screenshot:
  max_workers: 16
  timeout: 120
  batch_size: 100

# ==================== Agent Score Configuration ====================
agent_score:
  max_iter: 7
  window_width: 1920
  window_height: 1200
  timeout_seconds: 600
  batch_size: 3
  max_workers: 16

